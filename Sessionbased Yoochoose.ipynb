{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JjH3lqI4AdLg"},"outputs":[],"source":["# Part of the code adapted from https://github.com/CRIPAC-DIG/SR-GNN/blob/e21cfa431f74c25ae6e4ae9261deefe11d1cb488/pytorch_code/\n","# as well as https://github.com/userbehavioranalysis/SR-GNN_PyTorch-Geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24080,"status":"ok","timestamp":1682785166251,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"},"user_tz":240},"id":"VTYlH5oZ2yxS","outputId":"eafd7732-d54d-41b9-fc91-cf01fa36d791"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOILbGFP111S"},"outputs":[],"source":["import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0jkzqnR3nIA"},"outputs":[],"source":["import numpy as np\n","import networkx as nx"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11908,"status":"ok","timestamp":1682785179416,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"},"user_tz":240},"id":"rJdNNm02EQXq","outputId":"2fc360dc-931f-4fad-d153-f83cb8d4270d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch_geometric\n","  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n","Building wheels for collected packages: torch_geometric\n","  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910476 sha256=80d4e6c2022d9e1777480e94e12ab177b2bddb745d3c55405b7eff1eb8950d4a\n","  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n","Successfully built torch_geometric\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.3.1\n"]}],"source":["!pip install torch_geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3WTcz7HEelB"},"outputs":[],"source":["import torch_geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vV9-Xae-Nrf"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import torch_geometric.transforms as T\n","\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","import torch_geometric\n","from torch.nn import Parameter\n","from torch_geometric.utils.convert import to_networkx\n","import networkx as nx\n","import urllib.request\n","import tarfile\n","from torch_geometric.nn import GraphSAGE\n","from torch_geometric.nn import SAGEConv\n","from torch_geometric.nn import GATConv\n","from torch_geometric.loader import NeighborLoader\n","from torch_geometric.utils import to_networkx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNpC85pihcAg"},"outputs":[],"source":["from torch_geometric.data import InMemoryDataset, Data\n","\n","\n","class MultiSessionsGraph(InMemoryDataset):\n","    \"\"\"Every session is a graph.\"\"\"\n","    def __init__(self, root, phrase, transform=None, pre_transform=None):\n","        \"\"\"\n","        Args:\n","            root: 'sample', 'yoochoose1_4', 'yoochoose1_64' or 'diginetica'\n","            phrase: 'train' or 'test'\n","        \"\"\"\n","        assert phrase in ['train', 'test']\n","        self.phrase = phrase\n","        super(MultiSessionsGraph, self).__init__(root, transform, pre_transform)\n","        self.data, self.slices = torch.load(self.processed_paths[0])\n","     \n","    @property\n","    def raw_file_names(self):\n","        return [self.phrase + '.txt']\n","    \n","    @property\n","    def processed_file_names(self):\n","        return [self.phrase + '.pt']\n","    \n","    def download(self):\n","        pass\n","    \n","    def process(self):\n","        data = pickle.load(open(self.raw_dir + '/' + self.raw_file_names[0], 'rb'))\n","        data_list = []\n","        \n","        for sequences, y in zip(data[0], data[1]):\n","            i = 0\n","            nodes = {}    # dict{15: 0, 16: 1, 18: 2, ...}\n","            senders = []\n","            x = []\n","            for node in sequences:\n","                if node not in nodes:\n","                    nodes[node] = i\n","                    x.append([node])\n","                    i += 1\n","                senders.append(nodes[node])\n","            receivers = senders[:]\n","            del senders[-1]    # the last item is a receiver\n","            del receivers[0]    # the first item is a sender\n","            edge_index = torch.tensor([senders, receivers], dtype=torch.long)\n","            x = torch.tensor(x, dtype=torch.long)\n","            y = torch.tensor([y], dtype=torch.long)\n","            data_list.append(Data(x=x, edge_index=edge_index, y=y))\n","            \n","        data, slices = self.collate(data_list)\n","        torch.save((data, slices), self.processed_paths[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBFnHCfqvsEJ"},"outputs":[],"source":["n_node = 37483"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5XP4OkzukUIj"},"outputs":[],"source":["# If replicating the code, first run preprocess.py and then use the obtained file here\n","cur_dir = os.getcwd()\n","train_dataset = MultiSessionsGraph(cur_dir + '/drive/MyDrive/AM220proj/yoochoose1_64', phrase='train')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHY4JAiem96T"},"outputs":[],"source":["from torch_geometric.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682785188182,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"},"user_tz":240},"id":"PBghLUto73Rq","outputId":"29430e6c-729b-463c-f2db-f5be99381595"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]}],"source":["train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6cnchVD8CJh"},"outputs":[],"source":["test_dataset = MultiSessionsGraph(cur_dir + '/drive/MyDrive/AM220proj/yoochoose1_64', phrase='test')\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"il35PekCpRl4"},"outputs":[],"source":["import torch.nn as nn\n","import math\n","from torch_geometric.nn import GCNConv, GATConv, GatedGraphConv\n","from torch.nn import Linear\n","from torch_geometric.nn import TopKPooling\n","from torch_geometric.nn.glob import global_add_pool, global_mean_pool"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6C5jMIjupPHj"},"outputs":[],"source":["class GNNModel(nn.Module):\n","    \"\"\"\n","    Args:\n","        hidden_size: the number of units in a hidden layer.\n","        n_node: the number of items in the whole item set for embedding layer.\n","    \"\"\"\n","    def __init__(self, hidden_size, n_node):\n","        super(GNNModel, self).__init__()\n","        self.hidden_size, self.n_node = hidden_size, n_node\n","        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n","        self.conv2 = GCNConv(self.hidden_size, self.hidden_size)\n","        self.pool1 = TopKPooling(64, ratio=0.8)\n","        self.conv3 = GCNConv(self.hidden_size, self.hidden_size)\n","        self.pool3 = TopKPooling(64, ratio=0.8)\n","        self.lin = Linear(self.hidden_size, self.n_node)\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.reset_parameters()\n","        \n","    def reset_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.hidden_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = (data.x - 1), data.edge_index, data.batch\n","        # print(x, edge_index, batch, np.shape(x),np.shape(edge_index),np.shape(batch))\n","\n","        # print(np.shape(self.embedding(x)),\"firstlayer\")\n","\n","        x = self.embedding(x).squeeze()\n","\n","\n","        x = self.conv2(x, edge_index).relu()\n","\n","\n","        x = self.conv3(x, edge_index).relu()\n","\n","\n","        x = global_mean_pool(x, batch)\n","\n","        x = self.lin(x)\n","\n","        # embedding = self.embedding(x).squeeze()\n","        # hidden = self.gated(embedding, edge_index)\n","        # hidden2 = F.relu(hidden)\n","  \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nKDGpz71RXE"},"outputs":[],"source":["def forward(model, loader, device, epoch, top_k=20, optimizer=None, train_flag=True):\n","    if train_flag:\n","        model.train()\n","    else:\n","        model.eval()\n","        hit, mrr = [], []\n","\n","    mean_loss = 0.0\n","    updates_per_epoch = len(loader)\n","\n","    for i, batch in enumerate(loader):\n","        if train_flag:\n","            optimizer.zero_grad()\n","        # print(np.shape(batch))\n","        scores = model(batch.to(device))\n","        # print(np.shape(scores))\n","        targets = batch.y - 1\n","        loss = model.loss_function(scores.float(), targets)\n","        # loss_srgnn.append(loss)\n","\n","        if train_flag:\n","            loss.backward()\n","            optimizer.step()\n","            if i % 1500 == 0:\n","              print(\"Epoch: \", epoch, \"Batch \", i, \"loss: \", loss.item())\n","        else:\n","            sub_scores = scores.topk(top_k)[1]    # batch * top_k\n","            for score, target in zip(sub_scores.detach().cpu().numpy(), targets.detach().cpu().numpy()):\n","                hit.append(np.isin(target, score))\n","                if len(np.where(score == target)[0]) == 0:\n","                    mrr.append(0)\n","                else:\n","                    mrr.append(1 / (np.where(score == target)[0][0] + 1))\n","\n","        mean_loss += loss / batch.num_graphs\n","\n","    if train_flag:\n","      print(\"Epoch: \", epoch, \"train loss: \", mean_loss.item()/len(loader))\n","    else:\n","      hit = np.mean(hit) * 100\n","      mrr = np.mean(mrr) * 100\n","      print(\"Epoch: \", epoch, \"test loss: \", mean_loss.item()/len(loader))\n","      print(\"Epoch: \", epoch, \"Test hit\", hit, \"Test mrr\", mrr)\n","      return hit, mrr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFgYdyNq6FWS"},"outputs":[],"source":["device = 'cuda'\n","model = GNNModel(hidden_size=64, n_node=n_node).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","source":["import time"],"metadata":{"id":"w1PrLpw-pEet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["besthit = 0\n","start_time = time.time()\n","bestmrr = 0\n","loss_gcn = []\n","for epoch in range(10):\n","    forward(model, train_loader, device, epoch, optimizer=optimizer, train_flag=True)\n","    with torch.no_grad():\n","        hit, mrr = forward(model, test_loader, device, epoch,  train_flag=False)\n","        if hit >= besthit:\n","          besthit = hit\n","        if mrr >= bestmrr:\n","          bestmrr = mrr\n","print(\"time elapsed:\", time.time() - start_time)\n","print(f\"Best HIT@20: {besthit:.4f}, Best MRR@20: {bestmrr:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bzIycYSo9m4","executionInfo":{"status":"ok","timestamp":1682749650154,"user_tz":240,"elapsed":447195,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}},"outputId":"43609116-b40d-419b-a0b0-969f9161f262"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:  0 Batch  0 loss:  10.53860855102539\n","Epoch:  0 Batch  1500 loss:  7.0568108558654785\n","Epoch:  0 Batch  3000 loss:  6.565487861633301\n","Epoch:  0 Batch  4500 loss:  6.527947425842285\n","Epoch:  0 train loss:  0.10874190281006704\n","Epoch:  0 test loss:  0.09183677655881275\n","Epoch:  0 Test hit 42.590074779061865 Test mrr 14.37655000325036\n","Epoch:  1 Batch  0 loss:  5.779108047485352\n","Epoch:  1 Batch  1500 loss:  6.092808723449707\n","Epoch:  1 Batch  3000 loss:  6.104969024658203\n","Epoch:  1 Batch  4500 loss:  5.7630133628845215\n","Epoch:  1 train loss:  0.09222798594966479\n","Epoch:  1 test loss:  0.08756491794193363\n","Epoch:  1 Test hit 49.96779849010698 Test mrr 18.15954287113623\n","Epoch:  2 Batch  0 loss:  5.599277973175049\n","Epoch:  2 Batch  1500 loss:  5.22727632522583\n","Epoch:  2 Batch  3000 loss:  5.076815128326416\n","Epoch:  2 Batch  4500 loss:  6.171772003173828\n","Epoch:  2 train loss:  0.08493181908419388\n","Epoch:  2 test loss:  0.08800305818256579\n","Epoch:  2 Test hit 52.17002397223515 Test mrr 19.435563647822583\n","Epoch:  3 Batch  0 loss:  4.535053730010986\n","Epoch:  3 Batch  1500 loss:  5.516342639923096\n","Epoch:  3 Batch  3000 loss:  4.710900783538818\n","Epoch:  3 Batch  4500 loss:  5.385141849517822\n","Epoch:  3 train loss:  0.08023701176098887\n","Epoch:  3 test loss:  0.08965830508304133\n","Epoch:  3 Test hit 53.2648753085978 Test mrr 20.331150581663195\n","Epoch:  4 Batch  0 loss:  4.326292991638184\n","Epoch:  4 Batch  1500 loss:  4.599740982055664\n","Epoch:  4 Batch  3000 loss:  4.4138689041137695\n","Epoch:  4 Batch  4500 loss:  4.997136116027832\n","Epoch:  4 train loss:  0.07710432956787955\n","Epoch:  4 test loss:  0.0907330021978243\n","Epoch:  4 Test hit 53.80156714014813 Test mrr 20.682308296293485\n","Epoch:  5 Batch  0 loss:  4.518514156341553\n","Epoch:  5 Batch  1500 loss:  4.7827043533325195\n","Epoch:  5 Batch  3000 loss:  4.974966049194336\n","Epoch:  5 Batch  4500 loss:  4.973836898803711\n","Epoch:  5 train loss:  0.07483790836532223\n","Epoch:  5 test loss:  0.09213475940974954\n","Epoch:  5 Test hit 54.04128949157394 Test mrr 20.764517868629483\n","Epoch:  6 Batch  0 loss:  4.50184440612793\n","Epoch:  6 Batch  1500 loss:  5.148075103759766\n","Epoch:  6 Batch  3000 loss:  4.9381232261657715\n","Epoch:  6 Batch  4500 loss:  4.640724182128906\n","Epoch:  6 train loss:  0.07313251957348886\n","Epoch:  6 test loss:  0.09356232911552935\n","Epoch:  6 Test hit 54.109270456903644 Test mrr 20.826263094575335\n","Epoch:  7 Batch  0 loss:  4.725617408752441\n","Epoch:  7 Batch  1500 loss:  4.98475456237793\n","Epoch:  7 Batch  3000 loss:  4.330871105194092\n","Epoch:  7 Batch  4500 loss:  4.2069549560546875\n","Epoch:  7 train loss:  0.0717258387370918\n","Epoch:  7 test loss:  0.09459819837347594\n","Epoch:  7 Test hit 54.72288811764285 Test mrr 21.18067862848175\n","Epoch:  8 Batch  0 loss:  4.625594615936279\n","Epoch:  8 Batch  1500 loss:  4.472529888153076\n","Epoch:  8 Batch  3000 loss:  4.806610107421875\n","Epoch:  8 Batch  4500 loss:  4.871164798736572\n","Epoch:  8 train loss:  0.07042424819048713\n","Epoch:  8 test loss:  0.09587924704126144\n","Epoch:  8 Test hit 54.758667573079535 Test mrr 21.115916232056357\n","Epoch:  9 Batch  0 loss:  4.672295093536377\n","Epoch:  9 Batch  1500 loss:  4.248992443084717\n","Epoch:  9 Batch  3000 loss:  4.056031703948975\n","Epoch:  9 Batch  4500 loss:  4.931800365447998\n","Epoch:  9 train loss:  0.06948007761813366\n","Epoch:  9 test loss:  0.09726447971789187\n","Epoch:  9 Test hit 54.95903252352499 Test mrr 21.558202594384\n","time elapsed: 446.9247636795044\n","Best HIT@20: 54.9590, Best MRR@20: 21.5582\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7l--9xzVM50"},"outputs":[],"source":["# Next we evaluate GAT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsSZowXBVQZi"},"outputs":[],"source":["class AttentionModel(nn.Module):\n","    \"\"\"\n","    Args:\n","        hidden_size: the number of units in a hidden layer.\n","        n_node: the number of items in the whole item set for embedding layer.\n","    \"\"\"\n","    def __init__(self, hidden_size, n_node):\n","        super(AttentionModel, self).__init__()\n","        self.hidden_size, self.n_node = hidden_size, n_node\n","        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n","        self.conv2 = GATConv(self.hidden_size, self.hidden_size)\n","        self.pool1 = TopKPooling(64, ratio=0.8)\n","        self.conv3 = GATConv(self.hidden_size, self.hidden_size)\n","        self.pool3 = TopKPooling(64, ratio=0.8)\n","        self.lin = Linear(self.hidden_size, self.n_node)\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.reset_parameters()\n","        \n","    def reset_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.hidden_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = (data.x - 1), data.edge_index, data.batch\n","        # print(x, edge_index, batch, np.shape(x),np.shape(edge_index),np.shape(batch))\n","\n","        # print(np.shape(self.embedding(x)),\"firstlayer\")\n","\n","        x = self.embedding(x).squeeze()\n","\n","\n","        x = self.conv2(x, edge_index).relu()\n","\n","\n","        x = self.conv3(x, edge_index).relu()\n","\n","\n","        x = global_mean_pool(x, batch)\n","\n","        x = self.lin(x)\n","\n","        # embedding = self.embedding(x).squeeze()\n","        # hidden = self.gated(embedding, edge_index)\n","        # hidden2 = F.relu(hidden)\n","  \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIO-3KU8VfSS"},"outputs":[],"source":["device = 'cuda'\n","model = AttentionModel(hidden_size=64, n_node=n_node).to(device)"]},{"cell_type":"code","source":["besthit = 0\n","start_time = time.time()\n","bestmrr = 0\n","loss_gat = []\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(10):\n","    forward(model, train_loader, device, epoch, optimizer=optimizer, train_flag=True)\n","    with torch.no_grad():\n","        hit, mrr = forward(model, test_loader, device, epoch,  train_flag=False)\n","        if hit >= besthit:\n","          besthit = hit\n","        if mrr >= bestmrr:\n","          bestmrr = mrr\n","print(\"time elapsed:\", time.time() - start_time)\n","print(f\"Best HIT@20: {besthit:.4f}, Best MRR@20: {bestmrr:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s1QDKDqLtrcC","executionInfo":{"status":"ok","timestamp":1682750554016,"user_tz":240,"elapsed":521176,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}},"outputId":"13b0066c-be9a-4734-9ed5-a753776608bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:  0 Batch  0 loss:  10.54663372039795\n","Epoch:  0 Batch  1500 loss:  6.990447521209717\n","Epoch:  0 Batch  3000 loss:  6.3286213874816895\n","Epoch:  0 Batch  4500 loss:  6.841845512390137\n","Epoch:  0 train loss:  0.10735925588640788\n","Epoch:  0 test loss:  0.09043144906958399\n","Epoch:  0 Test hit 45.037389530931335 Test mrr 15.12693112960938\n","Epoch:  1 Batch  0 loss:  6.191983222961426\n","Epoch:  1 Batch  1500 loss:  5.18905782699585\n","Epoch:  1 Batch  3000 loss:  5.009959697723389\n","Epoch:  1 Batch  4500 loss:  5.54498815536499\n","Epoch:  1 train loss:  0.0905238379244161\n","Epoch:  1 test loss:  0.08621592925396881\n","Epoch:  1 Test hit 51.33994060610397 Test mrr 19.072901641539\n","Epoch:  2 Batch  0 loss:  6.023955821990967\n","Epoch:  2 Batch  1500 loss:  5.096771240234375\n","Epoch:  2 Batch  3000 loss:  4.853274822235107\n","Epoch:  2 Batch  4500 loss:  5.2791666984558105\n","Epoch:  2 train loss:  0.0829603201790252\n","Epoch:  2 test loss:  0.08664252065139177\n","Epoch:  2 Test hit 53.34001216501485 Test mrr 20.280114655164784\n","Epoch:  3 Batch  0 loss:  5.183353424072266\n","Epoch:  3 Batch  1500 loss:  5.1600213050842285\n","Epoch:  3 Batch  3000 loss:  4.973957061767578\n","Epoch:  3 Batch  4500 loss:  4.5883588790893555\n","Epoch:  3 train loss:  0.07830513818866242\n","Epoch:  3 test loss:  0.08783702719402531\n","Epoch:  3 Test hit 54.90894128591363 Test mrr 21.085072827421882\n","Epoch:  4 Batch  0 loss:  4.891915321350098\n","Epoch:  4 Batch  1500 loss:  4.903196811676025\n","Epoch:  4 Batch  3000 loss:  4.733592510223389\n","Epoch:  4 Batch  4500 loss:  5.139904975891113\n","Epoch:  4 train loss:  0.07522280999945934\n","Epoch:  4 test loss:  0.0897552874322887\n","Epoch:  4 Test hit 55.51898100110916 Test mrr 21.451060400871352\n","Epoch:  5 Batch  0 loss:  4.793173789978027\n","Epoch:  5 Batch  1500 loss:  4.7472357749938965\n","Epoch:  5 Batch  3000 loss:  4.433721542358398\n","Epoch:  5 Batch  4500 loss:  4.668325424194336\n","Epoch:  5 train loss:  0.07296076579902411\n","Epoch:  5 test loss:  0.09028165454995442\n","Epoch:  5 Test hit 55.97516905792694 Test mrr 21.8014963232159\n","Epoch:  6 Batch  0 loss:  4.456068515777588\n","Epoch:  6 Batch  1500 loss:  4.390118598937988\n","Epoch:  6 Batch  3000 loss:  4.336483001708984\n","Epoch:  6 Batch  4500 loss:  4.3490424156188965\n","Epoch:  6 train loss:  0.07123429981482483\n","Epoch:  6 test loss:  0.09161498290177614\n","Epoch:  6 Test hit 56.012737486135464 Test mrr 21.836166355303973\n","Epoch:  7 Batch  0 loss:  3.838435649871826\n","Epoch:  7 Batch  1500 loss:  4.616621017456055\n","Epoch:  7 Batch  3000 loss:  4.687655448913574\n","Epoch:  7 Batch  4500 loss:  4.256083011627197\n","Epoch:  7 train loss:  0.06992191723886246\n","Epoch:  7 test loss:  0.09288028394057495\n","Epoch:  7 Test hit 56.3061290207163 Test mrr 22.027247777091908\n","Epoch:  8 Batch  0 loss:  4.467653751373291\n","Epoch:  8 Batch  1500 loss:  4.3576340675354\n","Epoch:  8 Batch  3000 loss:  4.603852272033691\n","Epoch:  8 Batch  4500 loss:  4.612000465393066\n","Epoch:  8 train loss:  0.06886312095351697\n","Epoch:  8 test loss:  0.09398938316502342\n","Epoch:  8 Test hit 56.38305484990519 Test mrr 22.2005477206451\n","Epoch:  9 Batch  0 loss:  4.216368198394775\n","Epoch:  9 Batch  1500 loss:  3.673246383666992\n","Epoch:  9 Batch  3000 loss:  4.793572902679443\n","Epoch:  9 Batch  4500 loss:  4.286769390106201\n","Epoch:  9 train loss:  0.06804356558512652\n","Epoch:  9 test loss:  0.09549307223042852\n","Epoch:  9 Test hit 56.3937886865362 Test mrr 22.17439153581708\n","time elapsed: 520.7287166118622\n","Best HIT@20: 56.3938, Best MRR@20: 22.2005\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TeVGtGkEYzfK"},"outputs":[],"source":["# Next we try GraphSAGE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcmfdolDY38T"},"outputs":[],"source":["class SageModel(nn.Module):\n","    \"\"\"\n","    Args:\n","        hidden_size: the number of units in a hidden layer.\n","        n_node: the number of items in the whole item set for embedding layer.\n","    \"\"\"\n","    def __init__(self, hidden_size, n_node):\n","        super(SageModel, self).__init__()\n","        self.hidden_size, self.n_node = hidden_size, n_node\n","        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n","        self.conv2 = SAGEConv(self.hidden_size, self.hidden_size)\n","        self.pool1 = TopKPooling(64, ratio=0.8)\n","        self.conv3 = SAGEConv(self.hidden_size, self.hidden_size)\n","        self.pool3 = TopKPooling(64, ratio=0.8)\n","        self.lin = Linear(self.hidden_size, self.n_node)\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.reset_parameters()\n","        \n","    def reset_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.hidden_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = (data.x - 1), data.edge_index, data.batch\n","        # print(x, edge_index, batch, np.shape(x),np.shape(edge_index),np.shape(batch))\n","\n","        # print(np.shape(self.embedding(x)),\"firstlayer\")\n","\n","        x = self.embedding(x).squeeze()\n","\n","\n","        x = self.conv2(x, edge_index).relu()\n","\n","\n","        x = self.conv3(x, edge_index).relu()\n","\n","\n","        x = global_mean_pool(x, batch)\n","\n","        x = self.lin(x)\n","\n","        # embedding = self.embedding(x).squeeze()\n","        # hidden = self.gated(embedding, edge_index)\n","        # hidden2 = F.relu(hidden)\n","  \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BzEVG5EBY38V"},"outputs":[],"source":["device = 'cuda'\n","model = SageModel(hidden_size=64, n_node=n_node).to(device)"]},{"cell_type":"code","source":["besthit = 0\n","start_time = time.time()\n","bestmrr = 0\n","loss_sage = []\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(10):\n","    forward(model, train_loader, device, epoch, optimizer=optimizer, train_flag=True)\n","    with torch.no_grad():\n","        hit, mrr = forward(model, test_loader, device, epoch,  train_flag=False)\n","        if hit >= besthit:\n","          besthit = hit\n","        if mrr >= bestmrr:\n","          bestmrr = mrr\n","print(\"time elapsed:\", time.time() - start_time)\n","print(f\"Best HIT@20: {besthit:.4f}, Best MRR@20: {bestmrr:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JE0PMYeOwRtH","executionInfo":{"status":"ok","timestamp":1682751060672,"user_tz":240,"elapsed":414104,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}},"outputId":"3ed0b49e-0b5a-4424-f5fb-808fe43a0c0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:  0 Batch  0 loss:  10.532249450683594\n","Epoch:  0 Batch  1500 loss:  7.17373514175415\n","Epoch:  0 Batch  3000 loss:  6.62449836730957\n","Epoch:  0 Batch  4500 loss:  6.669823169708252\n","Epoch:  0 train loss:  0.10436256699083586\n","Epoch:  0 test loss:  0.08676075880805742\n","Epoch:  0 Test hit 50.187842141042616 Test mrr 17.5845922381155\n","Epoch:  1 Batch  0 loss:  5.990737438201904\n","Epoch:  1 Batch  1500 loss:  6.345929145812988\n","Epoch:  1 Batch  3000 loss:  5.12469482421875\n","Epoch:  1 Batch  4500 loss:  5.570568084716797\n","Epoch:  1 train loss:  0.08691341835734753\n","Epoch:  1 test loss:  0.08318619542591087\n","Epoch:  1 Test hit 56.282872374682455 Test mrr 21.576320700820137\n","Epoch:  2 Batch  0 loss:  4.448413372039795\n","Epoch:  2 Batch  1500 loss:  5.19327974319458\n","Epoch:  2 Batch  3000 loss:  5.017430782318115\n","Epoch:  2 Batch  4500 loss:  4.6934332847595215\n","Epoch:  2 train loss:  0.07972135972811689\n","Epoch:  2 test loss:  0.08349359717467283\n","Epoch:  2 Test hit 58.78743425525064 Test mrr 22.93993154342942\n","Epoch:  3 Batch  0 loss:  4.367895603179932\n","Epoch:  3 Batch  1500 loss:  5.32504940032959\n","Epoch:  3 Batch  3000 loss:  4.707004547119141\n","Epoch:  3 Batch  4500 loss:  4.835524559020996\n","Epoch:  3 train loss:  0.07514098804302281\n","Epoch:  3 test loss:  0.08425600414145183\n","Epoch:  3 Test hit 59.80357078965258 Test mrr 23.778754046188023\n","Epoch:  4 Batch  0 loss:  4.735876083374023\n","Epoch:  4 Batch  1500 loss:  4.426377773284912\n","Epoch:  4 Batch  3000 loss:  4.464975833892822\n","Epoch:  4 Batch  4500 loss:  4.01481294631958\n","Epoch:  4 train loss:  0.07167766003460208\n","Epoch:  4 test loss:  0.08641854467326499\n","Epoch:  4 Test hit 60.2776485741887 Test mrr 24.068440116628757\n","Epoch:  5 Batch  0 loss:  4.457859039306641\n","Epoch:  5 Batch  1500 loss:  5.128763198852539\n","Epoch:  5 Batch  3000 loss:  4.187154769897461\n","Epoch:  5 Batch  4500 loss:  4.7341413497924805\n","Epoch:  5 train loss:  0.06942362537846021\n","Epoch:  5 test loss:  0.08790276743454573\n","Epoch:  5 Test hit 60.80897348742352 Test mrr 24.389498335247403\n","Epoch:  6 Batch  0 loss:  4.4867448806762695\n","Epoch:  6 Batch  1500 loss:  4.439467906951904\n","Epoch:  6 Batch  3000 loss:  3.4762332439422607\n","Epoch:  6 Batch  4500 loss:  4.007730484008789\n","Epoch:  6 train loss:  0.06734717345980212\n","Epoch:  6 test loss:  0.08920084693611896\n","Epoch:  6 Test hit 60.65333285627392 Test mrr 24.36298161545231\n","Epoch:  7 Batch  0 loss:  4.31842565536499\n","Epoch:  7 Batch  1500 loss:  4.16680908203125\n","Epoch:  7 Batch  3000 loss:  4.105175018310547\n","Epoch:  7 Batch  4500 loss:  4.467578411102295\n","Epoch:  7 train loss:  0.06593848627743837\n","Epoch:  7 test loss:  0.08942745042883832\n","Epoch:  7 Test hit 60.49411427958067 Test mrr 24.38829632353569\n","Epoch:  8 Batch  0 loss:  3.5558319091796875\n","Epoch:  8 Batch  1500 loss:  3.8847293853759766\n","Epoch:  8 Batch  3000 loss:  3.920280694961548\n","Epoch:  8 Batch  4500 loss:  4.185451984405518\n","Epoch:  8 train loss:  0.06477684892172632\n","Epoch:  8 test loss:  0.09128560081499393\n","Epoch:  8 Test hit 60.65869977458943 Test mrr 24.48222204994595\n","Epoch:  9 Batch  0 loss:  4.3370842933654785\n","Epoch:  9 Batch  1500 loss:  3.474126100540161\n","Epoch:  9 Batch  3000 loss:  4.507420063018799\n","Epoch:  9 Batch  4500 loss:  4.026195526123047\n","Epoch:  9 train loss:  0.06343266576219182\n","Epoch:  9 test loss:  0.0930471180232766\n","Epoch:  9 Test hit 60.45654585137215 Test mrr 24.325557600669033\n","time elapsed: 413.6511707305908\n","Best HIT@20: 60.8090, Best MRR@20: 24.4822\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfvMRplcbfMD"},"outputs":[],"source":["# SR-GNN as proposed in https://arxiv.org/abs/1811.00855"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6RklyfTBbqmm"},"outputs":[],"source":["# The implementation of SR-GNN adopted from https://github.com/userbehavioranalysis/SR-GNN_PyTorch-Geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1hv28e8ivqJ"},"outputs":[],"source":["class Embedding2Score(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Embedding2Score, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.W_1 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.W_2 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.q = nn.Linear(self.hidden_size, 1)\n","        self.W_3 = nn.Linear(2 * self.hidden_size, self.hidden_size)\n","\n","    def forward(self, session_embedding, all_item_embedding, batch):\n","        sections = torch.bincount(batch)\n","        v_i = torch.split(session_embedding, tuple(sections.cpu().numpy()))    # split whole x back into graphs G_i\n","        v_n_repeat = tuple(nodes[-1].view(1, -1).repeat(nodes.shape[0], 1) for nodes in v_i)    # repeat |V|_i times for the last node embedding\n","\n","        # Eq(6)\n","        alpha = self.q(torch.sigmoid(self.W_1(torch.cat(v_n_repeat, dim=0)) + self.W_2(session_embedding)))    # |V|_i * 1\n","        s_g_whole = alpha * session_embedding    # |V|_i * hidden_size\n","        s_g_split = torch.split(s_g_whole, tuple(sections.cpu().numpy()))    # split whole s_g into graphs G_i\n","        s_g = tuple(torch.sum(embeddings, dim=0).view(1, -1) for embeddings in s_g_split)\n","        \n","        # Eq(7)\n","        v_n = tuple(nodes[-1].view(1, -1) for nodes in v_i)\n","        s_h = self.W_3(torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1))\n","        \n","        # Eq(8)\n","        z_i_hat = torch.mm(s_h, all_item_embedding.weight.transpose(1, 0))\n","        \n","        return z_i_hat\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNrfLv2bi0-w"},"outputs":[],"source":["class SRGNNModel(nn.Module):\n","    \"\"\"\n","    Args:\n","        hidden_size: the number of units in a hidden layer.\n","        n_node: the number of items in the whole item set for embedding layer.\n","    \"\"\"\n","    def __init__(self, hidden_size, n_node):\n","        super(SRGNNModel, self).__init__()\n","        self.hidden_size, self.n_node = hidden_size, n_node\n","        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n","        self.gated = GatedGraphConv(self.hidden_size, num_layers=1)\n","        self.e2s = Embedding2Score(self.hidden_size)\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.reset_parameters()\n","        \n","    def reset_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.hidden_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, data):\n","        x, edge_index, batch = data.x - 1, data.edge_index, data.batch\n","\n","        embedding = self.embedding(x).squeeze()\n","        hidden = self.gated(embedding, edge_index)\n","        hidden2 = F.relu(hidden)\n","  \n","        return self.e2s(hidden2, self.embedding, batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NULKcSFIi8CB"},"outputs":[],"source":["device = 'cuda'\n","model = SRGNNModel(hidden_size=64, n_node=n_node).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Hx7gk5GjCug","outputId":"301e5d4c-d0f0-4736-a731-dcc0c1d115e6","executionInfo":{"status":"ok","timestamp":1682752321860,"user_tz":240,"elapsed":981873,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:  0 Batch  0 loss:  10.540619850158691\n","Epoch:  0 Batch  1500 loss:  6.941524982452393\n","Epoch:  0 Batch  3000 loss:  4.65255880355835\n","Epoch:  0 Batch  4500 loss:  5.3174943923950195\n","Epoch:  0 train loss:  0.09347086447745459\n","Epoch:  0 test loss:  0.0752538598102072\n","Epoch:  0 Test hit 63.47275394468495 Test mrr 25.50715651893541\n","Epoch:  1 Batch  0 loss:  4.603010654449463\n","Epoch:  1 Batch  1500 loss:  5.073220729827881\n","Epoch:  1 Batch  3000 loss:  4.496406555175781\n","Epoch:  1 Batch  4500 loss:  3.6636247634887695\n","Epoch:  1 train loss:  0.07315508040589858\n","Epoch:  1 test loss:  0.07183805155808648\n","Epoch:  1 Test hit 66.81634405524348 Test mrr 27.525610926601583\n","Epoch:  2 Batch  0 loss:  4.192098140716553\n","Epoch:  2 Batch  1500 loss:  4.29026985168457\n","Epoch:  2 Batch  3000 loss:  3.8466243743896484\n","Epoch:  2 Batch  4500 loss:  4.131102561950684\n","Epoch:  2 train loss:  0.06829721523403709\n","Epoch:  2 test loss:  0.07032028652164975\n","Epoch:  2 Test hit 68.10440445096425 Test mrr 28.320487732461736\n","Epoch:  3 Batch  0 loss:  5.007205009460449\n","Epoch:  3 Batch  1500 loss:  3.5625948905944824\n","Epoch:  3 Batch  3000 loss:  4.289239406585693\n","Epoch:  3 Batch  4500 loss:  4.590554237365723\n","Epoch:  3 train loss:  0.06574926673334774\n","Epoch:  3 test loss:  0.0704225326293666\n","Epoch:  3 Test hit 68.27435686428852 Test mrr 28.661435208860382\n","Epoch:  4 Batch  0 loss:  4.202815532684326\n","Epoch:  4 Batch  1500 loss:  4.108932971954346\n","Epoch:  4 Batch  3000 loss:  3.8539085388183594\n","Epoch:  4 Batch  4500 loss:  3.8067617416381836\n","Epoch:  4 train loss:  0.06388323826773357\n","Epoch:  4 test loss:  0.07081323793878272\n","Epoch:  4 Test hit 68.3816952305986 Test mrr 28.609771348028108\n","Epoch:  5 Batch  0 loss:  4.385592460632324\n","Epoch:  5 Batch  1500 loss:  3.618096351623535\n","Epoch:  5 Batch  3000 loss:  4.730239391326904\n","Epoch:  5 Batch  4500 loss:  3.999953031539917\n","Epoch:  5 train loss:  0.06239078614126027\n","Epoch:  5 test loss:  0.07095549472136684\n","Epoch:  5 Test hit 68.05610218612472 Test mrr 29.073253118496833\n","Epoch:  6 Batch  0 loss:  3.1753313541412354\n","Epoch:  6 Batch  1500 loss:  3.706312417984009\n","Epoch:  6 Batch  3000 loss:  3.7859628200531006\n","Epoch:  6 Batch  4500 loss:  4.002499103546143\n","Epoch:  6 train loss:  0.06100271680363322\n","Epoch:  6 test loss:  0.07190174041678213\n","Epoch:  6 Test hit 67.92192922823715 Test mrr 28.838420831703782\n","Epoch:  7 Batch  0 loss:  4.00869083404541\n","Epoch:  7 Batch  1500 loss:  3.7934770584106445\n","Epoch:  7 Batch  3000 loss:  3.7990806102752686\n","Epoch:  7 Batch  4500 loss:  4.210476398468018\n","Epoch:  7 train loss:  0.05987117991727941\n","Epoch:  7 test loss:  0.07219784210693918\n","Epoch:  7 Test hit 67.63032666642815 Test mrr 28.81741558701774\n","Epoch:  8 Batch  0 loss:  3.1815202236175537\n","Epoch:  8 Batch  1500 loss:  3.666501522064209\n","Epoch:  8 Batch  3000 loss:  3.9183855056762695\n","Epoch:  8 Batch  4500 loss:  3.9276843070983887\n","Epoch:  8 train loss:  0.058777842802159926\n","Epoch:  8 test loss:  0.07304442992745057\n","Epoch:  8 Test hit 67.38523739668682 Test mrr 28.736348013022706\n","Epoch:  9 Batch  0 loss:  3.578126907348633\n","Epoch:  9 Batch  1500 loss:  3.716484308242798\n","Epoch:  9 Batch  3000 loss:  4.3521223068237305\n","Epoch:  9 Batch  4500 loss:  3.2885334491729736\n","Epoch:  9 train loss:  0.05782488654641544\n","Epoch:  9 test loss:  0.07341205937365919\n","Epoch:  9 Test hit 67.24033060216823 Test mrr 28.720034650218384\n","time elapsed: 981.4575853347778\n","Best HIT@20: 68.3817, Best MRR@20: 29.0733\n"]}],"source":["besthit = 0\n","start_time = time.time()\n","bestmrr = 0\n","loss_srgnn = []\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(10):\n","    forward(model, train_loader, device, epoch, optimizer=optimizer, train_flag=True)\n","    with torch.no_grad():\n","        hit, mrr = forward(model, test_loader, device, epoch,  train_flag=False)\n","        if hit >= besthit:\n","          besthit = hit\n","        if mrr >= bestmrr:\n","          bestmrr = mrr\n","print(\"time elapsed:\", time.time() - start_time)\n","print(f\"Best HIT@20: {besthit:.4f}, Best MRR@20: {bestmrr:.4f}\")"]},{"cell_type":"code","source":["# import pickle\n","# data = {\n","#     \"loss_sage\": loss_sage,\n","#     \"loss_gcn\": loss_gcn,\n","#     \"loss_gat\": loss_gat,\n","#     \"loss_srgnn\": loss_srgnn\n","# }\n","# !touch data.pkl\n","# with open(\"data.pkl\", \"wb\") as f:\n","#     pickle.dump(data, f)"],"metadata":{"id":"lx_3xHLJ6CrD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IwnIN_1-6vgh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Next we consider framing this as a hypergraph problem"],"metadata":{"id":"HXTMJkDk5GvC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = pickle.load(open('/content/drive/MyDrive/AM220proj/yoochoose1_64/raw/train.txt', 'rb'))\n","test_data = pickle.load(open('/content/drive/MyDrive/AM220proj/yoochoose1_64/raw/test.txt', 'rb'))"],"metadata":{"id":"hOPM0O2KX2hM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# code adapted from https://github.com/wangjlgz/Hypergraph-Session-Recommendation/blob/main/main.py"],"metadata":{"id":"0E7mciAbcrKE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def translation(data, item_dic):\n","\n","    datax = []\n","    for i in range(len(data[0])):\n","        datax.append([item_dic[s] for s in data[0][i]])\n","    datay = [item_dic[s] for s in data[1]]\n","\n","    return (datax, datay)\n","\n","class Data():\n","    def __init__(self, data, window):\n","        inputs = data[0]\n","        self.inputs = np.asarray(inputs) \n","        self.targets = np.asarray(data[1])\n","        self.length = len(inputs)\n","        self.window = window\n","\n","\n","    def generate_batch(self, batch_size, shuffle = False):\n","        if shuffle:\n","            shuffled_arg = np.arange(self.length)\n","            np.random.shuffle(shuffled_arg)\n","            self.inputs = self.inputs[shuffled_arg]\n","            self.targets = self.targets[shuffled_arg]\n","        n_batch = int(self.length / batch_size)\n","        if self.length % batch_size != 0:\n","            n_batch += 1\n","        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n","        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n","        return slices\n","\n","    def get_slice(self, iList):\n","        inputs, targets = self.inputs[iList], self.targets[iList]\n","        items, n_node, H, HT, G, EG, alias_inputs, node_masks, node_dic = [], [], [], [], [], [], [], [], []\n","        num_edge, edge_mask, edge_inputs = [], [], []\n","\n","        for u_input in inputs:\n","            temp_s = u_input\n","            \n","            temp_l = list(set(temp_s))    \n","            temp_dic = {temp_l[i]: i for i in range(len(temp_l))}        \n","            n_node.append(temp_l)\n","            alias_inputs.append([temp_dic[i] for i in temp_s])\n","            node_dic.append(temp_dic)\n","\n","            min_s = min(self.window, len(u_input))\n","            num_edge.append(int((1 + min_s) * len(u_input) - (1 + min_s) * min_s / 2))\n","\n","\n","        max_n_node = np.max([len(i) for i in n_node])\n","\n","        max_n_edge = max(num_edge)\n","\n","        max_se_len = max([len(i) for i in alias_inputs])\n","\n","        edge_mask = [[1] * len(le) + [0] * (max_n_edge - len(le)) for le in alias_inputs]\n","\n","        for idx in range(len(inputs)):\n","            u_input = inputs[idx]\n","            effect_len = len(alias_inputs[idx])\n","            node = n_node[idx]\n","            items.append(node + (max_n_node - len(node)) * [0])\n","\n","            effect_list = alias_inputs[idx]\n","            ws = np.ones(max_n_edge)\n","            cols = []\n","            rows = []\n","            edg = []\n","            e_idx = 0\n","\n","            for w in range(1 + min(self.window, effect_len-1)):\n","                edge_idx = list(np.arange(e_idx, e_idx + effect_len-w))\n","                edg += edge_idx\n","                for ww in range(w + 1):\n","                    rows += effect_list[ww:ww+effect_len-w]\n","                    cols += edge_idx\n","\n","                e_idx += len(edge_idx)\n","\n","\n","            u_H = sp.coo_matrix(([1.0]*len(rows), (rows, cols)), shape=(max_n_node, max_n_edge))\n","            HT.append(np.asarray(u_H.T.todense()))\n","\n","\n","            node_masks.append((max_se_len - len(alias_inputs[idx])) * [0] + [1]*len(alias_inputs[idx]))\n","            alias_inputs[idx] = (max_se_len - len(alias_inputs[idx])) * [0] + alias_inputs[idx]\n","\n","\n","            edge_inputs.append(edg + (max_n_edge - len(edg))*[0])\n","\n","        return alias_inputs, H, HT, G, EG, items, targets, node_masks, edge_mask, edge_inputs"],"metadata":{"id":"C2IDJDiDc44W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llen = [len(train_data[0][i]) for i in range(len(train_data[0]))] + [len(test_data[0][i]) for i in range(len(test_data[0]))]\n","print(max(llen),sum(llen)*1.0/len(llen))\n","l = []\n","for i in range(len(train_data[0])):\n","    l += list(train_data[0][i])\n","l += list(train_data[1])\n","\n","for i in range(len(test_data[0])):\n","    l += list(test_data[0][i])\n","l += list(test_data[1])\n","l = set(l)\n","print('total number of items', len(l))\n","\n","item_dic = {}\n","for i in l:\n","    item_dic[i] = len(item_dic) + 1 #start from 1\n","\n","del l\n","train_data = translation(train_data, item_dic)\n","test_data = translation(test_data, item_dic)\n","\n","n_node = len(item_dic) + 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86wxu0r2cods","executionInfo":{"status":"ok","timestamp":1682040962350,"user_tz":240,"elapsed":1779,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}},"outputId":"7d8a18bd-ab90-4037-e064-b5c0379ea0d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["145 5.158024412986751\n","total number of items 17745\n"]}]},{"cell_type":"code","source":["train_data = Data(train_data, 1)\n","test_data = Data(test_data, 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ZvPJ1y3dNcT","executionInfo":{"status":"ok","timestamp":1682041032443,"user_tz":240,"elapsed":3,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}},"outputId":"c8dabe1d-adb1-48cb-cac7-769d8d131ff7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-89-8dc76b540c74>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  self.inputs = np.asarray(inputs)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","\n","class EncoderLayer(nn.Module):\n","    ''' Compose with two layers '''\n","\n","    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n","        super(EncoderLayer, self).__init__()\n","        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n","        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n","\n","    def forward(self, enc_input, slf_attn_mask=None):\n","        enc_output, enc_slf_attn = self.slf_attn(\n","            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n","        enc_output = self.pos_ffn(enc_output)\n","        return enc_output, enc_slf_attn\n","\n","class MultiHeadAttention(nn.Module):\n","    ''' Multi-Head Attention module '''\n","\n","    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n","        super().__init__()\n","\n","        self.dropout = dropout\n","\n","        self.n_head = n_head\n","        self.d_k = d_k\n","        self.d_v = d_v\n","\n","        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n","        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n","        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n","        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n","\n","        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5, attn_dropout = self.dropout)\n","\n","        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n","\n","\n","    def forward(self, q, k, v, mask=None):\n","\n","        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n","        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n","\n","        residual = q\n","        q = self.layer_norm(q)\n","\n","        # Pass through the pre-attention projection: b x lq x (n*dv)\n","        # Separate different heads: b x lq x n x dv\n","        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n","        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n","        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n","\n","        # Transpose for attention dot product: b x n x lq x dv\n","        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n","        \n","        if mask is not None:\n","            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n","\n","        q, attn = self.attention(q, k, v, mask=mask)\n","\n","        # Transpose to move the head dimension back: b x lq x n x dv\n","        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n","        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n","        q = F.dropout(self.fc(q), self.dropout, training=self.training)\n","        q += residual\n","\n","        return q, attn\n","\n","\n","class PositionwiseFeedForward(nn.Module):\n","    ''' A two-feed-forward-layer module '''\n","\n","    def __init__(self, d_in, d_hid, dropout=0.1):\n","        super().__init__()\n","        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n","        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n","        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n","        self.dropout = dropout\n","\n","    def forward(self, x):\n","\n","        residual = x\n","        x = self.layer_norm(x)\n","\n","        x = self.w_2(F.relu(self.w_1(x)))\n","        x = F.dropout(x, self.dropout, training=self.training)\n","        x += residual\n","\n","        return x\n","\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_hid, n_position=200):\n","        super(PositionalEncoding, self).__init__()\n","\n","        # Not a parameter\n","        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n","\n","    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n","        ''' Sinusoid position encoding table '''\n","        # TODO: make it with torch instead of numpy\n","\n","        def get_position_angle_vec(position):\n","            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n","\n","        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n","        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n","        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n","\n","        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n","\n","    def forward(self, x):\n","        return x + self.pos_table[:, :x.size(1)].clone().detach()\n","\n","\n","class ScaledDotProductAttention(nn.Module):\n","    ''' Scaled Dot-Product Attention '''\n","\n","    def __init__(self, temperature, attn_dropout=0.1):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.dropout = attn_dropout\n","\n","    def forward(self, q, k, v, mask=None):\n","\n","        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n","\n","        if mask is not None:\n","            attn = attn.masked_fill(mask == 0, -1e9)\n","\n","        #print(F.softmax(attn, dim=-1))\n","\n","        attn =  F.dropout(F.softmax(attn, dim=-1), self.dropout, training=self.training)\n","        output = torch.matmul(attn, v)\n","\n","        return output, attn\n"],"metadata":{"id":"8fmKtjsndojg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","\n","\n","\n","class ScaledDotProductAttention_hyper(nn.Module):\n","    ''' Scaled Dot-Product Attention for Hypergraph'''\n","\n","    def __init__(self, temperature, attn_dropout=0.1):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.dropout = attn_dropout\n","\n","    def forward(self, q, k, v, mask=None):\n","\n","        attn = torch.matmul(q / self.temperature, k.transpose(1, 2))\n","\n","        if mask is not None:\n","            attn = attn.masked_fill(mask == 0, -1e9)\n","\n","        attn =  F.dropout(F.softmax(attn, dim=-1), self.dropout, training=self.training)\n","        output = torch.matmul(attn, v)\n","\n","        return output, attn\n","\n","class HyperGraphAttentionLayerSparse(nn.Module):\n","\n","    def __init__(self, in_features, out_features, dropout, alpha, transfer, concat=True, bias=False):\n","        super(HyperGraphAttentionLayerSparse, self).__init__()\n","        self.dropout = dropout\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.alpha = alpha\n","        self.concat = concat\n","\n","\n","        self.transfer = transfer\n","\n","        if self.transfer:\n","            self.weight = Parameter(torch.Tensor(self.in_features, self.out_features))\n","        else:\n","            self.register_parameter('weight', None)\n","\n","        self.weight2 = Parameter(torch.Tensor(self.in_features, self.out_features))\n","        self.weight3 = Parameter(torch.Tensor(self.out_features, self.out_features))\n","\n","        if bias:\n","            self.bias = Parameter(torch.Tensor(self.out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","\n","        self.word_context = nn.Embedding(1, self.out_features)\n","      \n","       \n","        self.leakyrelu = nn.LeakyReLU(self.alpha)\n","\n","        self.attention1 = ScaledDotProductAttention_hyper(temperature=self.out_features ** 0.5, attn_dropout = self.dropout)\n","        self.attention2 = ScaledDotProductAttention_hyper(temperature=self.out_features ** 0.5, attn_dropout = self.dropout)\n","        \n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        stdv = 1. / math.sqrt(self.out_features)\n","        if self.weight is not None:\n","            self.weight.data.uniform_(-stdv, stdv)\n","        self.weight2.data.uniform_(-stdv, stdv)\n","        self.weight3.data.uniform_(-stdv, stdv)\n","        if self.bias is not None:\n","            self.bias.data.uniform_(-stdv, stdv)\n","        \n","        nn.init.uniform_(self.word_context.weight.data, -stdv, stdv)\n","\n","\n","    def forward(self, x, adj):\n","        residual = x\n","\n","        x_4att = x.matmul(self.weight2)\n","\n","\n","        if self.transfer:\n","            x = x.matmul(self.weight)\n","            if self.bias is not None:\n","                x = x + self.bias        \n","\n","        N1 = adj.shape[1] #number of edge\n","        N2 = adj.shape[2] #number of node\n","\n","\n","        q1 = self.word_context.weight[0:].view(1, 1, -1).repeat(x.shape[0], N1, 1).view(x.shape[0], N1, self.out_features)\n","        edge, att1 = self.attention1(q1, x_4att, x, mask = adj) \n","\n","\n","        edge_4att = edge.matmul(self.weight3)\n","      \n","        node, attn = self.attention2(x_4att, edge_4att, edge, mask = adj.transpose(1, 2)) \n","\n","\n","        if self.concat:\n","\n","            node = F.relu(node)\n","            edge = F.relu(edge)\n","\n","        return node, edge\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"],"metadata":{"id":"Xo8fL0RQdnTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime\n","import math\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.nn import Module, Parameter\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","\n","def get_pad_mask(seq, pad_idx):\n","    return (seq != pad_idx).unsqueeze(-2)\n","\n","\n","\n","class HGNN_ATT(nn.Module):\n","    def __init__(self, dataset, input_size, n_hid, output_size, step, dropout=0.3):\n","        super(HGNN_ATT, self).__init__()\n","        self.dropout = dropout\n","        self.step = step\n","        self.dataset = dataset\n","        self.gat1 = HyperGraphAttentionLayerSparse(input_size, n_hid, self.dropout, 0.2, transfer=False, concat=False)\n","        self.gat2 = HyperGraphAttentionLayerSparse(n_hid, output_size, self.dropout, 0.2, transfer=True,  concat=False)\n","        \n","    def forward(self, x, H, G, EG):   \n","\n","        residual = x\n","\n","        x,y = self.gat1(x, H)\n","\n","        if self.step == 2:\n","\n","            x = F.dropout(x, self.dropout, training=self.training)\n","            x += residual\n","            x,y = self.gat2(x, H)\n","\n","        x = F.dropout(x, self.dropout, training=self.training)\n","        x += residual\n","\n","        return x, x\n","\n","\n","\n","class SessionGraph(Module):\n","    def __init__(self, opt, n_node):\n","        super(SessionGraph, self).__init__()\n","        self.hidden_size = opt.hiddenSize\n","        self.n_node = n_node\n","        self.batch_size = opt.batchSize\n","        self.nonhybrid = opt.nonhybrid\n","        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n","        self.embedding2 = nn.Embedding(self.n_node, self.hidden_size)\n","        self.dropout = opt.dropout\n","        self.dataset = opt.dataset\n","        # for self-attention\n","        n_layers = 1\n","        n_head = 1\n","   \n","        \n","        self.layer_norm = nn.LayerNorm(self.hidden_size, eps=1e-6)\n","        self.layer_norm1 = nn.LayerNorm(self.hidden_size, eps=1e-6)\n","\n","        self.layer_stack = nn.ModuleList([\n","            EncoderLayer(self.hidden_size, self.hidden_size, n_head, self.hidden_size, self.hidden_size, dropout=opt.dropout)\n","            for _ in range(n_layers)])\n","\n","        self.reset_parameters()\n","        \n","\n","        self.hgnn = HGNN_ATT(self.dataset, self.hidden_size, self.hidden_size, self.hidden_size, opt.step, dropout = self.dropout)\n","\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2)\n","        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n","\n","    def reset_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.hidden_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def compute_scores(self, enc_output, enc_output2, mask, edge_mask, hidden):\n","\n","\n","\n","        for enc_layer in self.layer_stack:\n","            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask = get_pad_mask(mask, 0))\n","                    \n","        ht = enc_output[torch.arange(mask.shape[0]).long(), mask.shape[1]-1]  # batch_size x latent_size\n","\n","        ht = self.layer_norm(ht)\n","\n","        hidden = ht\n","\n","        b = self.embedding.weight[1:]  # n_nodes x latent_size\n","        scores = torch.matmul(hidden, b.transpose(1, 0))\n","\n","        return scores\n","\n","\n","\n","\n","    def forward(self, inputs, HT, G, EG): \n","        nodes = self.embedding(inputs) \n","        #nodes = self.layer_norm1(nodes)       \n","        nodes, hidden = self.hgnn(nodes, HT, G, EG)\n","        nodes2 = self.embedding2(inputs) \n","        return nodes,hidden,nodes2\n","\n","\n","def trans_to_cuda(variable):\n","    if torch.cuda.is_available():\n","        return variable.cuda()\n","    else:\n","        return variable\n","\n","\n","def trans_to_cpu(variable):\n","    if torch.cuda.is_available():\n","        return variable.cpu()\n","    else:\n","        return variable\n","\n","\n","def forward(model, alias_inputs, H, HT, G, EG, items, targets, node_masks, edge_mask, edge_inputs):\n","    \n","    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n","    items = trans_to_cuda(torch.Tensor(items).long())\n","    HT = trans_to_cuda(torch.Tensor(HT).float())\n","    G = trans_to_cuda(torch.Tensor(G).float())\n","    EG = trans_to_cuda(torch.Tensor(EG).float())\n","    node_masks = trans_to_cuda(torch.Tensor(node_masks).long())\n","    edge_mask = trans_to_cuda(torch.Tensor(edge_mask).long())\n","    nodes, hidden, nodes2 = model(items, HT, G, EG)\n","    get = lambda i: nodes[i][alias_inputs[i]]\n","    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n","\n","    get2 = lambda i: nodes2[i][alias_inputs[i]]\n","    seq_hidden2 = torch.stack([get2(i) for i in torch.arange(len(alias_inputs)).long()])\n","    return targets, model.compute_scores(seq_hidden, seq_hidden2, node_masks, edge_mask, hidden)\n","\n","\n","def train_model(model, train_data, opt):\n","    model.scheduler.step()\n","    print('start training: ', datetime.datetime.now())\n","    model.train()\n","    total_loss = 0.0\n","    slices = train_data.generate_batch(opt.batchSize, True)\n","    for step in tqdm(range(len(slices)), total=len(slices), ncols=70, leave=False, unit='b'):\n","        i = slices[step]\n","        alias_inputs, H, HT, G, EG, items, targets, node_masks, edge_mask, edge_inputs = train_data.get_slice(i)    \n","        model.optimizer.zero_grad()\n","        targets, scores = forward(model, alias_inputs, H, HT, G, EG, items, targets, node_masks, edge_mask, edge_inputs)\n","        targets = trans_to_cuda(torch.Tensor(targets).long())\n","        loss = model.loss_function(scores, targets - 1)\n","        loss.backward()\n","        model.optimizer.step()\n","        total_loss += loss\n","    print('\\tLoss:\\t%.3f' % total_loss)\n","\n","def test_model(model, test_data, opt):\n","    \n","    model.eval()\n","    hit20, mrr20, hit10, mrr10 = [], [], [], []\n","    slices = test_data.generate_batch(min(128,test_data.length), False)\n","    for step in tqdm(range(len(slices)), total=len(slices), ncols=70, leave=False, unit='b'):\n","        i = slices[step]\n","        alias_inputs, H, HT, G, EG, items, targets, node_masks, edge_mask, edge_inputs = test_data.get_slice(i)\n","        targets, scores = forward(model, alias_inputs, H, HT, G, EG, items, targets, node_masks, edge_mask, edge_inputs)\n","        sub_scores = scores.topk(20)[1]\n","        sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n","\n","        for score, target in zip(sub_scores, targets):\n","            hit20.append(np.isin(target - 1, score))\n","            if len(np.where(score == target - 1)[0]) == 0:\n","                mrr20.append(0)\n","            else:\n","                mrr20.append(1.0 / (np.where(score == target - 1)[0][0] + 1))\n","\n","            hit10.append(np.isin(target - 1, score[:10]))\n","            if len(np.where(score[:10] == target - 1)[0]) == 0:\n","                mrr10.append(0)\n","            else:\n","                mrr10.append(1.0 / (np.where(score[:10] == target - 1)[0][0] + 1))\n","    hit20 = np.mean(hit20) * 100\n","    mrr20 = np.mean(mrr20) * 100\n","    hit10 = np.mean(hit10) * 100\n","    mrr10 = np.mean(mrr10) * 100\n","    return hit20, mrr20, hit10, mrr10"],"metadata":{"id":"NMGM3NcOdVc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import pickle as pkl\n","import networkx as nx\n","import scipy.sparse as sp\n","from nltk.corpus import stopwords\n","import nltk\n","from nltk.wsd import lesk\n","from nltk.corpus import wordnet as wn\n","from scipy.sparse.linalg import eigs, eigsh\n","import sys\n","import re\n","import collections\n","from collections import Counter\n","import numpy as np\n","from multiprocessing import Process, Queue\n","import pandas as pd\n","import os\n","import random\n","from argparse import Namespace\n","\n","opt = Namespace(dataset='yoochoose1_64', batchSize=100, hiddenSize=100, epoch=5, worker=3, lr=0.001, lr_dc=0.1, lr_dc_step=3, l2=0.0, step=2, window=1, patience=3, nonhybrid=False, validation=False, valid_portion=0.1, dropout=0.3)\n","model = trans_to_cuda(SessionGraph(opt, n_node))\n","\n","start = time.time()\n","best_result = [0, 0, 0, 0]\n","best_epoch = [0, 0]\n","bad_counter = 0\n","\n","\n","\n","\n","for epoch in range(opt.epoch):\n","    print('-------------------------------------------------------')\n","    print('epoch: ', epoch)\n","\n","    train_model(model, train_data, opt)\n","\n","    hit, mrr, hit10, mrr10 = test_model(model, test_data, opt)\n","\n","\n","    flag = 0\n","    if hit >= best_result[0]:\n","        best_result[0] = hit\n","        best_result[2] = hit10\n","        best_epoch[0] = epoch\n","        flag = 1\n","    if mrr >= best_result[1]:\n","        best_result[1] = mrr\n","        best_result[3] = mrr10\n","        best_epoch[1] = epoch\n","        flag = 1\n","\n","    print('Result:\\n')\n","    print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tHIT@10:\\t%.4f\\tMRR@10:\\t%.4f\\tEpoch:\\t%d\\n'% (hit, mrr, hit10, mrr10, epoch))\n","\n","    print('Best Result:')\n","    print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tHIT@10:\\t%.4f\\tMRR@10:\\t%.4f\\tEpoch:\\t%d\\n'% (best_result[0], best_result[1], best_result[2], best_result[3], best_epoch[0]))\n","    bad_counter += 1 - flag\n","    if bad_counter >= opt.patience:\n","        break\n","print('-------------------------------------------------------')\n","end = time.time()\n","print(\"Run time: %f s\" % (end - start))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJQRw8tXdyXJ","executionInfo":{"status":"ok","timestamp":1682047437408,"user_tz":240,"elapsed":5850092,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}},"outputId":"45477706-0384-4609-d3a1-1b38318af021"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------------------\n","epoch:  0\n","start training:  2023-04-21 01:46:26.991403\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tLoss:\t20454.527\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Result:\n","\n","\tRecall@20:\t68.7037\tMMR@20:\t29.4960\tHIT@10:\t58.1863\tMRR@10:\t28.7538\tEpoch:\t0\n","\n","Best Result:\n","\tRecall@20:\t68.7037\tMMR@20:\t29.4960\tHIT@10:\t58.1863\tMRR@10:\t28.7538\tEpoch:\t0\n","\n","-------------------------------------------------------\n","epoch:  1\n","start training:  2023-04-21 02:06:00.046000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tLoss:\t16202.825\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Result:\n","\n","\tRecall@20:\t70.1170\tMMR@20:\t30.2697\tHIT@10:\t59.4028\tMRR@10:\t29.5118\tEpoch:\t1\n","\n","Best Result:\n","\tRecall@20:\t70.1170\tMMR@20:\t30.2697\tHIT@10:\t59.4028\tMRR@10:\t29.5118\tEpoch:\t1\n","\n","-------------------------------------------------------\n","epoch:  2\n","start training:  2023-04-21 02:25:32.483958\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tLoss:\t14783.261\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Result:\n","\n","\tRecall@20:\t70.9471\tMMR@20:\t31.2187\tHIT@10:\t60.7660\tMRR@10:\t30.5006\tEpoch:\t2\n","\n","Best Result:\n","\tRecall@20:\t70.9471\tMMR@20:\t31.2187\tHIT@10:\t60.7660\tMRR@10:\t30.5006\tEpoch:\t2\n","\n","-------------------------------------------------------\n","epoch:  3\n","start training:  2023-04-21 02:44:59.147623\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tLoss:\t14542.952\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Result:\n","\n","\tRecall@20:\t70.9864\tMMR@20:\t31.4660\tHIT@10:\t60.6802\tMRR@10:\t30.7367\tEpoch:\t3\n","\n","Best Result:\n","\tRecall@20:\t70.9864\tMMR@20:\t31.4660\tHIT@10:\t60.6802\tMRR@10:\t30.7367\tEpoch:\t3\n","\n","-------------------------------------------------------\n","epoch:  4\n","start training:  2023-04-21 03:04:22.192918\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\tLoss:\t14401.284\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Result:\n","\n","\tRecall@20:\t71.0866\tMMR@20:\t31.5683\tHIT@10:\t60.7786\tMRR@10:\t30.8395\tEpoch:\t4\n","\n","Best Result:\n","\tRecall@20:\t71.0866\tMMR@20:\t31.5683\tHIT@10:\t60.7786\tMRR@10:\t30.8395\tEpoch:\t4\n","\n","-------------------------------------------------------\n","Run time: 5849.861582 s\n"]}]},{"cell_type":"code","source":["'''\n","Hypergraph networks following implementation in \n","https://github.com/xiaxin1998/DHCN/blob/main/main.py\n","'''"],"metadata":{"id":"yl_opVEL407J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip uninstall torch --yes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p0cYr8rd45dx","executionInfo":{"status":"ok","timestamp":1682283199898,"user_tz":240,"elapsed":23619,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}},"outputId":"281b7fe1-f13e-4f86-d0c6-18917b848273"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.0.0+cu118\n","Uninstalling torch-2.0.0+cu118:\n","  Successfully uninstalled torch-2.0.0+cu118\n"]}]},{"cell_type":"code","source":["!pip install torch==1.7.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XZEs5cEH5EU1","executionInfo":{"status":"ok","timestamp":1682283278214,"user_tz":240,"elapsed":29849,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}},"outputId":"dc676d5c-2efb-438e-8ffe-c093ed1e4b45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.7.1\n","  Downloading torch-1.7.1-cp39-cp39-manylinux1_x86_64.whl (776.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.7.1) (4.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch==1.7.1) (1.22.4)\n","Installing collected packages: torch\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.7.1 which is incompatible.\n","torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.7.1 which is incompatible.\n","torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.7.1 which is incompatible.\n","torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.7.1\n"]}]},{"cell_type":"code","source":["!pip uninstall numpy --yes\n","!pip install numpy==1.18.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQu2YrNS5WNt","executionInfo":{"status":"ok","timestamp":1682283441253,"user_tz":240,"elapsed":147943,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}},"outputId":"cadaa77d-2503-48c1-cbce-8f82049adf37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 1.22.4\n","Uninstalling numpy-1.22.4:\n","  Successfully uninstalled numpy-1.22.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting numpy==1.18.1\n","  Downloading numpy-1.18.1.zip (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: numpy\n","  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for numpy: filename=numpy-1.18.1-cp39-cp39-linux_x86_64.whl size=13113811 sha256=bdea0c82caf45d7f4cef62f3617dd31279d594c4fc54dda947a098c47b95b141\n","  Stored in directory: /root/.cache/pip/wheels/d7/8f/69/b233132b552877f7f613530ca919c4f566aec2b4c88119979f\n","Successfully built numpy\n","Installing collected packages: numpy\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.18.1 which is incompatible.\n","xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.18.1 which is incompatible.\n","torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.7.1 which is incompatible.\n","torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.7.1 which is incompatible.\n","tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.18.1 which is incompatible.\n","tables 3.8.0 requires numpy>=1.19.0, but you have numpy 1.18.1 which is incompatible.\n","scipy 1.10.1 requires numpy<1.27.0,>=1.19.5, but you have numpy 1.18.1 which is incompatible.\n","plotnine 0.10.1 requires numpy>=1.19.0, but you have numpy 1.18.1 which is incompatible.\n","pandas 1.5.3 requires numpy>=1.20.3; python_version < \"3.10\", but you have numpy 1.18.1 which is incompatible.\n","opencv-python 4.7.0.72 requires numpy>=1.19.3; python_version >= \"3.9\", but you have numpy 1.18.1 which is incompatible.\n","opencv-python-headless 4.7.0.72 requires numpy>=1.19.3; python_version >= \"3.9\", but you have numpy 1.18.1 which is incompatible.\n","opencv-contrib-python 4.7.0.72 requires numpy>=1.19.3; python_version >= \"3.9\", but you have numpy 1.18.1 which is incompatible.\n","ml-dtypes 0.1.0 requires numpy>1.20, but you have numpy 1.18.1 which is incompatible.\n","mizani 0.8.1 requires numpy>=1.19.0, but you have numpy 1.18.1 which is incompatible.\n","matplotlib 3.7.1 requires numpy>=1.20, but you have numpy 1.18.1 which is incompatible.\n","librosa 0.10.0.post2 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you have numpy 1.18.1 which is incompatible.\n","jaxlib 0.4.7+cuda11.cudnn86 requires numpy>=1.21, but you have numpy 1.18.1 which is incompatible.\n","jax 0.4.8 requires numpy>=1.21, but you have numpy 1.18.1 which is incompatible.\n","gensim 4.3.1 requires numpy>=1.18.5, but you have numpy 1.18.1 which is incompatible.\n","cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.18.1 which is incompatible.\n","cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.18.1 which is incompatible.\n","astropy 5.2.2 requires numpy>=1.20, but you have numpy 1.18.1 which is incompatible.\n","arviz 0.15.1 requires numpy>=1.20.0, but you have numpy 1.18.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.18.1\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from scipy.sparse import csr_matrix\n","from operator import itemgetter\n","\n","def data_masks(all_sessions, n_node):\n","    indptr, indices, data = [], [], []\n","    indptr.append(0)\n","    for j in range(len(all_sessions)):\n","        session = np.unique(all_sessions[j])\n","        length = len(session)\n","        s = indptr[-1]\n","        indptr.append((s + length))\n","        for i in range(length):\n","            indices.append(session[i]-1)\n","            data.append(1)\n","    matrix = csr_matrix((data, indices, indptr), shape=(len(all_sessions), n_node))\n","\n","    return matrix\n","\n","def split_validation(train_set, valid_portion):\n","    train_set_x, train_set_y = train_set\n","    n_samples = len(train_set_x)\n","    sidx = np.arange(n_samples, dtype='int32')\n","    np.random.shuffle(sidx)\n","    n_train = int(np.round(n_samples * (1. - valid_portion)))\n","    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n","    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n","    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n","    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n","\n","    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)\n","\n","class Data():\n","    def __init__(self, data, shuffle=False, n_node=None):\n","        self.raw = np.asarray(data[0])\n","        H_T = data_masks(self.raw, n_node)\n","        BH_T = H_T.T.multiply(1.0/H_T.sum(axis=1).reshape(1, -1))\n","        BH_T = BH_T.T\n","        H = H_T.T\n","        DH = H.T.multiply(1.0/H.sum(axis=1).reshape(1, -1))\n","        DH = DH.T\n","        DHBH_T = np.dot(DH,BH_T)\n","\n","        self.adjacency = DHBH_T.tocoo()\n","        self.n_node = n_node\n","        self.targets = np.asarray(data[1])\n","        self.length = len(self.raw)\n","        self.shuffle = shuffle\n","\n","    def get_overlap(self, sessions):\n","        matrix = np.zeros((len(sessions), len(sessions)))\n","        for i in range(len(sessions)):\n","            seq_a = set(sessions[i])\n","            seq_a.discard(0)\n","            for j in range(i+1, len(sessions)):\n","                seq_b = set(sessions[j])\n","                seq_b.discard(0)\n","                overlap = seq_a.intersection(seq_b)\n","                ab_set = seq_a | seq_b\n","                matrix[i][j] = float(len(overlap))/float(len(ab_set))\n","                matrix[j][i] = matrix[i][j]\n","        matrix = matrix + np.diag([1.0]*len(sessions))\n","        degree = np.sum(np.array(matrix), 1)\n","        degree = np.diag(1.0/degree)\n","        return matrix, degree\n","\n","    def generate_batch(self, batch_size):\n","        if self.shuffle:\n","            shuffled_arg = np.arange(self.length)\n","            np.random.shuffle(shuffled_arg)\n","            self.raw = self.raw[shuffled_arg]\n","            self.targets = self.targets[shuffled_arg]\n","        n_batch = int(self.length / batch_size)\n","        if self.length % batch_size != 0:\n","            n_batch += 1\n","        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n","        slices[-1] = np.arange(self.length-batch_size, self.length)\n","        return slices\n","\n","    def get_slice(self, index):\n","        items, num_node = [], []\n","        inp = self.raw[index]\n","        for session in inp:\n","            num_node.append(len(np.nonzero(session)[0]))\n","        max_n_node = np.max(num_node)\n","        session_len = []\n","        reversed_sess_item = []\n","        mask = []\n","        for session in inp:\n","            nonzero_elems = np.nonzero(session)[0]\n","            session_len.append([len(nonzero_elems)])\n","            items.append(session + (max_n_node - len(nonzero_elems)) * [0])\n","            mask.append([1]*len(nonzero_elems) + (max_n_node - len(nonzero_elems)) * [0])\n","            reversed_sess_item.append(list(reversed(session)) + (max_n_node - len(nonzero_elems)) * [0])\n","\n","\n","        return self.targets[index]-1, session_len,items, reversed_sess_item, mask"],"metadata":{"id":"jDJqiazb0t3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime\n","import math\n","import numpy as np\n","import torch\n","from torch import nn, backends\n","from torch.nn import Module, Parameter\n","import torch.nn.functional as F\n","import torch.sparse\n","from scipy.sparse import coo\n","import time\n","from numba import jit\n","import heapq\n","\n","def trans_to_cuda(variable):\n","    if torch.cuda.is_available():\n","        return variable.cuda()\n","    else:\n","        return variable\n","def trans_to_cpu(variable):\n","    if torch.cuda.is_available():\n","        return variable.cpu()\n","    else:\n","        return variable\n","\n","class HyperConv(Module):\n","    def __init__(self, layers,dataset,emb_size=100):\n","        super(HyperConv, self).__init__()\n","        self.emb_size = emb_size\n","        self.layers = layers\n","        self.dataset = dataset\n","\n","    def forward(self, adjacency, embedding):\n","        item_embeddings = embedding\n","        item_embedding_layer0 = item_embeddings\n","        final = [item_embedding_layer0]\n","        for i in range(self.layers):\n","            item_embeddings = torch.sparse.mm(trans_to_cuda(adjacency), item_embeddings)\n","            final.append(item_embeddings)\n","        final = torch.stack(final,dim=0)\n","      #  final1 = trans_to_cuda(torch.tensor([item.cpu().detach().numpy() for item in final]))\n","      #  item_embeddings = torch.sum(final1, 0)\n","        item_embeddings = torch.sum(final, dim=0) / (self.layers+1)\n","        return item_embeddings\n","\n","\n","class LineConv(Module):\n","    def __init__(self, layers,batch_size,emb_size=100):\n","        super(LineConv, self).__init__()\n","        self.emb_size = emb_size\n","        self.batch_size = batch_size\n","        self.layers = layers\n","    def forward(self, item_embedding, D, A, session_item, session_len):\n","        zeros = torch.cuda.FloatTensor(1,self.emb_size).fill_(0)\n","        # zeros = torch.zeros([1,self.emb_size])\n","        item_embedding = torch.cat([zeros, item_embedding], 0)\n","        seq_h = []\n","        for i in torch.arange(len(session_item)):\n","            seq_h.append(torch.index_select(item_embedding, 0, session_item[i]))\n","        seq_h1 = trans_to_cuda(torch.tensor([item.cpu().detach().numpy() for item in seq_h]))\n","        session_emb_lgcn = torch.div(torch.sum(seq_h1, 1), session_len)\n","        session = [session_emb_lgcn]\n","        DA = torch.mm(D, A).float()\n","        for i in range(self.layers):\n","            session_emb_lgcn = torch.mm(DA, session_emb_lgcn)\n","            session.append(session_emb_lgcn)\n","        session = torch.stack(session, dim = 0)\n","        #session1 = trans_to_cuda(torch.tensor([item.cpu().detach().numpy() for item in session]))\n","        #session_emb_lgcn = torch.sum(session1, 0)\n","        session_emb_lgcn = torch.sum(session, dim=0)/ (self.layers+1)\n","        return session_emb_lgcn\n","\n","\n","class DHCN(Module):\n","    def __init__(self, adjacency, n_node,lr, layers,l2, beta,dataset,emb_size=100, batch_size=100):\n","        super(DHCN, self).__init__()\n","        self.emb_size = emb_size\n","        self.batch_size = batch_size\n","        self.n_node = n_node\n","        self.L2 = l2\n","        self.lr = lr\n","        self.layers = layers\n","        self.beta = beta\n","        self.dataset = dataset\n","\n","        values = adjacency.data\n","        indices = np.vstack((adjacency.row, adjacency.col))\n","        if dataset == 'Nowplaying':\n","            index_fliter = (values < 0.05).nonzero()\n","            values = np.delete(values, index_fliter)\n","            indices1 = np.delete(indices[0], index_fliter)\n","            indices2 = np.delete(indices[1], index_fliter)\n","            indices = [indices1, indices2]\n","        i = torch.LongTensor(indices)\n","        v = torch.FloatTensor(values)\n","        shape = adjacency.shape\n","        adjacency = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n","        self.adjacency = adjacency\n","        self.embedding = nn.Embedding(self.n_node, self.emb_size)\n","        self.pos_embedding = nn.Embedding(200, self.emb_size)\n","        self.HyperGraph = HyperConv(self.layers,dataset)\n","        self.LineGraph = LineConv(self.layers, self.batch_size)\n","        self.w_1 = nn.Linear(2 * self.emb_size, self.emb_size)\n","        self.w_2 = nn.Parameter(torch.Tensor(self.emb_size, 1))\n","        self.glu1 = nn.Linear(self.emb_size, self.emb_size)\n","        self.glu2 = nn.Linear(self.emb_size, self.emb_size, bias=False)\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n","        self.init_parameters()\n","\n","    def init_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.emb_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","     \n","    def generate_sess_emb(self,item_embedding, session_item, session_len, reversed_sess_item, mask):\n","        zeros = torch.cuda.FloatTensor(1, self.emb_size).fill_(0)\n","        # zeros = torch.zeros(1, self.emb_size)\n","        item_embedding = torch.cat([zeros, item_embedding], 0)\n","        get = lambda i: item_embedding[reversed_sess_item[i]]\n","        seq_h = torch.cuda.FloatTensor(self.batch_size, list(reversed_sess_item.shape)[1], self.emb_size).fill_(0)\n","        # seq_h = torch.zeros(self.batch_size, list(reversed_sess_item.shape)[1], self.emb_size)\n","        for i in torch.arange(session_item.shape[0]):\n","            seq_h[i] = get(i)\n","        hs = torch.div(torch.sum(seq_h, 1), session_len)\n","        mask = mask.float().unsqueeze(-1)\n","        len = seq_h.shape[1]\n","        pos_emb = self.pos_embedding.weight[:len]\n","        pos_emb = pos_emb.unsqueeze(0).repeat(self.batch_size, 1, 1)\n","\n","        hs = hs.unsqueeze(-2).repeat(1, len, 1)\n","        nh = self.w_1(torch.cat([pos_emb, seq_h], -1))\n","        nh = torch.tanh(nh)\n","        nh = torch.sigmoid(self.glu1(nh) + self.glu2(hs))\n","        beta = torch.matmul(nh, self.w_2)\n","        beta = beta * mask\n","        select = torch.sum(beta * seq_h, 1)\n","        return select\n","\n","    def generate_sess_emb_npos(self,item_embedding, session_item, session_len, reversed_sess_item, mask):\n","        zeros = torch.cuda.FloatTensor(1, self.emb_size).fill_(0)\n","        # zeros = torch.zeros(1, self.emb_size)\n","        item_embedding = torch.cat([zeros, item_embedding], 0)\n","        get = lambda i: item_embedding[reversed_sess_item[i]]\n","        seq_h = torch.cuda.FloatTensor(self.batch_size, list(reversed_sess_item.shape)[1], self.emb_size).fill_(0)\n","        # seq_h = torch.zeros(self.batch_size, list(reversed_sess_item.shape)[1], self.emb_size)\n","        for i in torch.arange(session_item.shape[0]):\n","            seq_h[i] = get(i)\n","        hs = torch.div(torch.sum(seq_h, 1), session_len)\n","        mask = mask.float().unsqueeze(-1)\n","        len = seq_h.shape[1]\n","        # pos_emb = self.pos_embedding.weight[:len]\n","        # pos_emb = pos_emb.unsqueeze(0).repeat(self.batch_size, 1, 1)\n","\n","        hs = hs.unsqueeze(-2).repeat(1, len, 1)\n","        nh = seq_h\n","        nh = torch.tanh(nh)\n","        nh = torch.sigmoid(self.glu1(nh) + self.glu2(hs))\n","        beta = torch.matmul(nh, self.w_2)\n","        beta = beta * mask\n","        select = torch.sum(beta * seq_h, 1)\n","        return select\n","\n","    def SSL(self, sess_emb_hgnn, sess_emb_lgcn):\n","        def row_shuffle(embedding):\n","            corrupted_embedding = embedding[torch.randperm(embedding.size()[0])]\n","            return corrupted_embedding\n","        def row_column_shuffle(embedding):\n","            corrupted_embedding = embedding[torch.randperm(embedding.size()[0])]\n","            corrupted_embedding = corrupted_embedding[:,torch.randperm(corrupted_embedding.size()[1])]\n","            return corrupted_embedding\n","        def score(x1, x2):\n","            return torch.sum(torch.mul(x1, x2), 1)\n","\n","        pos = score(sess_emb_hgnn, sess_emb_lgcn)\n","        neg1 = score(sess_emb_lgcn, row_column_shuffle(sess_emb_hgnn))\n","        one = torch.cuda.FloatTensor(neg1.shape[0]).fill_(1)\n","        # one = zeros = torch.ones(neg1.shape[0])\n","        con_loss = torch.sum(-torch.log(1e-8 + torch.sigmoid(pos))-torch.log(1e-8 + (one - torch.sigmoid(neg1))))\n","        return con_loss\n","\n","    def forward(self, session_item, session_len, D, A, reversed_sess_item, mask):\n","        item_embeddings_hg = self.HyperGraph(self.adjacency, self.embedding.weight)\n","        if self.dataset == 'Tmall':\n","            sess_emb_hgnn = self.generate_sess_emb_npos(item_embeddings_hg, session_item, session_len, reversed_sess_item, mask)\n","        else:\n","            sess_emb_hgnn = self.generate_sess_emb(item_embeddings_hg, session_item, session_len, reversed_sess_item, mask)\n","        session_emb_lg = self.LineGraph(self.embedding.weight, D, A, session_item, session_len)\n","        con_loss = self.SSL(sess_emb_hgnn, session_emb_lg)\n","        return item_embeddings_hg, sess_emb_hgnn, self.beta*con_loss\n","\n","\n","@jit(nopython=True)\n","def find_k_largest(K, candidates):\n","    n_candidates = []\n","    for iid, score in enumerate(candidates[:K]):\n","        n_candidates.append((score, iid))\n","    heapq.heapify(n_candidates)\n","    for iid, score in enumerate(candidates[K:]):\n","        if score > n_candidates[0][0]:\n","            heapq.heapreplace(n_candidates, (score, iid + K))\n","    n_candidates.sort(key=lambda d: d[0], reverse=True)\n","    ids = [item[1] for item in n_candidates]\n","    # k_largest_scores = [item[0] for item in n_candidates]\n","    return ids#, k_largest_scores\n","\n","def forward(model, i, data):\n","    tar, session_len, session_item, reversed_sess_item, mask = data.get_slice(i)\n","    A_hat, D_hat = data.get_overlap(session_item)\n","    session_item = trans_to_cuda(torch.Tensor(session_item).long())\n","    session_len = trans_to_cuda(torch.Tensor(session_len).long())\n","    A_hat = trans_to_cuda(torch.Tensor(A_hat))\n","    D_hat = trans_to_cuda(torch.Tensor(D_hat))\n","    tar = trans_to_cuda(torch.Tensor(tar).long())\n","    mask = trans_to_cuda(torch.Tensor(mask).long())\n","    reversed_sess_item = trans_to_cuda(torch.Tensor(reversed_sess_item).long())\n","    item_emb_hg, sess_emb_hgnn, con_loss = model(session_item, session_len, D_hat, A_hat, reversed_sess_item, mask)\n","    scores = torch.mm(sess_emb_hgnn, torch.transpose(item_emb_hg, 1,0))\n","    return tar, scores, con_loss\n","\n","\n","def train_test(model, train_data, test_data):\n","    print('start training: ', datetime.datetime.now())\n","    torch.autograd.set_detect_anomaly(True)\n","    total_loss = 0.0\n","    slices = train_data.generate_batch(model.batch_size)\n","    for i in slices:\n","        model.zero_grad()\n","        targets, scores, con_loss = forward(model, i, train_data)\n","        loss = model.loss_function(scores + 1e-8, targets)\n","        loss = loss + con_loss\n","        loss.backward()\n","#        print(loss.item())\n","        model.optimizer.step()\n","        total_loss += loss\n","    print('\\tLoss:\\t%.3f' % total_loss)\n","    top_K = [5, 10, 20]\n","    metrics = {}\n","    for K in top_K:\n","        metrics['hit%d' % K] = []\n","        metrics['mrr%d' % K] = []\n","    print('start predicting: ', datetime.datetime.now())\n","\n","    model.eval()\n","    slices = test_data.generate_batch(model.batch_size)\n","    for i in slices:\n","        tar, scores, con_loss = forward(model, i, test_data)\n","        scores = trans_to_cpu(scores).detach().numpy()\n","        index = []\n","        for idd in range(model.batch_size):\n","            index.append(find_k_largest(20, scores[idd]))\n","        index = np.array(index)\n","        tar = trans_to_cpu(tar).detach().numpy()\n","        for K in top_K:\n","            for prediction, target in zip(index[:, :K], tar):\n","                metrics['hit%d' %K].append(np.isin(target, prediction))\n","                if len(np.where(prediction == target)[0]) == 0:\n","                    metrics['mrr%d' %K].append(0)\n","                else:\n","                    metrics['mrr%d' %K].append(1 / (np.where(prediction == target)[0][0]+1))\n","    return metrics, total_loss\n","\n"],"metadata":{"id":"GVl9Kop224jX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = pickle.load(open('/content/drive/MyDrive/AM220proj/yoochoose1_64/raw/train.txt', 'rb'))\n","test_data = pickle.load(open('/content/drive/MyDrive/AM220proj/yoochoose1_64/raw/test.txt', 'rb'))\n","train_data = Data(train_data, shuffle=True, n_node=n_node)\n","test_data = Data(test_data, shuffle=True, n_node=n_node)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uSgB_yqK08dR","executionInfo":{"status":"ok","timestamp":1682288940284,"user_tz":240,"elapsed":5123,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}},"outputId":"25c01b7b-3505-418b-80f3-1165e6bc06aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-91-849acb4c4df6>:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  self.raw = np.asarray(data[0])\n","<ipython-input-91-849acb4c4df6>:40: RuntimeWarning: divide by zero encountered in true_divide\n","  DH = H.T.multiply(1.0/H.sum(axis=1).reshape(1, -1))\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","import heapq"],"metadata":{"id":"5v0oacdE3KO6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda'"],"metadata":{"id":"RQ9He0w78LjB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_node = 37483\n","model = trans_to_cuda(DHCN(adjacency=train_data.adjacency,n_node=n_node,lr=0.001, l2=1e-5, beta=0.02, layers=3,emb_size=100, batch_size=100,dataset=\"yoochoose\"))\n","\n","top_K = [5, 10, 20]\n","best_results = {}\n","for K in top_K:\n","    best_results['epoch%d' % K] = [0, 0]\n","    best_results['metric%d' % K] = [0, 0]\n","\n","for epoch in range(5):\n","    print('-------------------------------------------------------')\n","    print('epoch: ', epoch)\n","    metrics, total_loss = train_test(model, train_data, test_data)\n","    for K in top_K:\n","        metrics['hit%d' % K] = np.mean(metrics['hit%d' % K]) * 100\n","        metrics['mrr%d' % K] = np.mean(metrics['mrr%d' % K]) * 100\n","        if best_results['metric%d' % K][0] < metrics['hit%d' % K]:\n","            best_results['metric%d' % K][0] = metrics['hit%d' % K]\n","            best_results['epoch%d' % K][0] = epoch\n","        if best_results['metric%d' % K][1] < metrics['mrr%d' % K]:\n","            best_results['metric%d' % K][1] = metrics['mrr%d' % K]\n","            best_results['epoch%d' % K][1] = epoch\n","    print(metrics)\n","    for K in top_K:\n","        print('train_loss:\\t%.4f\\tRecall@%d: %.4f\\tMRR%d: %.4f\\tEpoch: %d,  %d' %\n","              (total_loss, K, best_results['metric%d' % K][0], K, best_results['metric%d' % K][1],\n","                best_results['epoch%d' % K][0], best_results['epoch%d' % K][1]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGrPE7Hm1AKC","executionInfo":{"status":"ok","timestamp":1682297856667,"user_tz":240,"elapsed":2475839,"user":{"displayName":"Jeff Jiang","userId":"05345923776204115994"}},"outputId":"2979d115-e040-44bf-915c-865e6a099d9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------------------\n","epoch:  0\n","start training:  2023-04-23 22:29:10.126486\n","\tLoss:\t24483.811\n","start predicting:  2023-04-23 22:55:14.917037\n","{'hit5': 44.28980322003578, 'mrr5': 26.108050089445438, 'hit10': 57.11091234347049, 'mrr10': 27.828169633983592, 'hit20': 67.89982110912344, 'mrr20': 28.589821653201152}\n","train_loss:\t24483.8105\tRecall@5: 44.2898\tMRR5: 26.1081\tEpoch: 0,  0\n","train_loss:\t24483.8105\tRecall@10: 57.1109\tMRR10: 27.8282\tEpoch: 0,  0\n","train_loss:\t24483.8105\tRecall@20: 67.8998\tMRR20: 28.5898\tEpoch: 0,  0\n","-------------------------------------------------------\n","epoch:  1\n","start training:  2023-04-23 22:58:37.934138\n","\tLoss:\t22489.121\n","start predicting:  2023-04-23 23:24:51.903041\n","{'hit5': 45.180679785330945, 'mrr5': 26.6468992248062, 'hit10': 58.0572450805009, 'mrr10': 28.37053695658347, 'hit20': 68.80500894454383, 'mrr20': 29.126848679693}\n","train_loss:\t22489.1211\tRecall@5: 45.1807\tMRR5: 26.6469\tEpoch: 1,  1\n","train_loss:\t22489.1211\tRecall@10: 58.0572\tMRR10: 28.3705\tEpoch: 1,  1\n","train_loss:\t22489.1211\tRecall@20: 68.8050\tMRR20: 29.1268\tEpoch: 1,  1\n","-------------------------------------------------------\n","epoch:  2\n","start training:  2023-04-23 23:28:14.789077\n","\tLoss:\t22194.570\n","start predicting:  2023-04-23 23:54:37.452531\n","{'hit5': 45.38461538461539, 'mrr5': 26.862283840190816, 'hit10': 58.22540250447227, 'mrr10': 28.587648010903827, 'hit20': 68.88372093023256, 'mrr20': 29.34041300883303}\n","train_loss:\t22194.5703\tRecall@5: 45.3846\tMRR5: 26.8623\tEpoch: 2,  2\n","train_loss:\t22194.5703\tRecall@10: 58.2254\tMRR10: 28.5876\tEpoch: 2,  2\n","train_loss:\t22194.5703\tRecall@20: 68.8837\tMRR20: 29.3404\tEpoch: 2,  2\n","-------------------------------------------------------\n","epoch:  3\n","start training:  2023-04-23 23:57:56.124819\n","\tLoss:\t22017.227\n","start predicting:  2023-04-24 00:24:15.719914\n","{'hit5': 45.395348837209305, 'mrr5': 26.886076326774, 'hit10': 58.18604651162791, 'mrr10': 28.603166794445862, 'hit20': 68.71198568872987, 'mrr20': 29.346736666312616}\n","train_loss:\t22017.2266\tRecall@5: 45.3953\tMRR5: 26.8861\tEpoch: 3,  3\n","train_loss:\t22017.2266\tRecall@10: 58.2254\tMRR10: 28.6032\tEpoch: 2,  3\n","train_loss:\t22017.2266\tRecall@20: 68.8837\tMRR20: 29.3467\tEpoch: 2,  3\n","-------------------------------------------------------\n","epoch:  4\n","start training:  2023-04-24 00:27:39.507454\n","\tLoss:\t21907.352\n","start predicting:  2023-04-24 00:54:15.228069\n","{'hit5': 45.36672629695885, 'mrr5': 26.832081097197374, 'hit10': 57.91771019677996, 'mrr10': 28.519963369963374, 'hit20': 68.57602862254025, 'mrr20': 29.274265212702243}\n","train_loss:\t21907.3516\tRecall@5: 45.3953\tMRR5: 26.8861\tEpoch: 3,  3\n","train_loss:\t21907.3516\tRecall@10: 58.2254\tMRR10: 28.6032\tEpoch: 2,  3\n","train_loss:\t21907.3516\tRecall@20: 68.8837\tMRR20: 29.3467\tEpoch: 2,  3\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPLcDaNtmbUT+0Toa3nSg2+"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}